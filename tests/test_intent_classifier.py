import os
import sys
import yaml
import pytest
import numpy as np
import pandas as pd
import tensorflow as tf
from dotenv import load_dotenv
from intent_classifier import IntentClassifier, Config
from sklearn.metrics import classification_report, confusion_matrix

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

# --- Fixtures (Contextos de Teste) ---
@pytest.fixture(scope="session")
def paths(request):
    """Fornece caminhos para arquivos de dados de teste, pulando se n√£o encontrados."""
    model_name = getattr(request, "param", "confusion")

    test_dir = os.path.dirname(__file__)
    project_root = os.path.abspath(os.path.join(test_dir, ".."))

    if model_name == "confusion":
        examples_path = os.path.join(project_root, "intent_classifier", "data", "confusion_intents.yml")
        config_path = os.path.join(project_root, "intent_classifier", "models", "confusion-v1_config.yml")
    elif model_name == "clair":
        examples_path = os.path.join(project_root, "intent_classifier", "data", "clair_intents.yml")
        config_path = os.path.join(project_root, "intent_classifier", "models", "clair-v1_config.yml")
    else:
        pytest.fail(f"Nome de modelo desconhecido: {model_name}")  
    
    if not (os.path.exists(examples_path)):
        pytest.skip(f"Arquivos de dados de teste n√£o encontrados em {examples_path}")
    if not (os.path.exists(config_path)):
        pytest.skip(f"Arquivos de configura√ß√£o n√£o encontrados em {config_path}")
        
    return {"config": config_path, "examples": examples_path}

@pytest.fixture(scope="module")
def clf_wandb(paths, request):
    """
    Fixture de integra√ß√£o: Carrega o modelo real do W&B.
    """
    model_name = getattr(request, "param", "confusion")
    if model_name == "confusion":
        env_url_var = "WANDB_CONFUSION_MODEL_URL"
    elif model_name == "clair":
        env_url_var = "WANDB_CLAIR_MODEL_URL"
    else:
        pytest.fail(f"Par√¢metro de modelo desconhecido: {model_name}")

    load_dotenv()
    env_url = os.getenv(env_url_var)
    api_key = os.getenv("WANDB_API_KEY")

    if not api_key:
        pytest.skip("WANDB_API_KEY n√£o definido. Pulando teste de integra√ß√£o W&B.")
    if not env_url:
        pytest.skip("WANDB_MODEL_URL n√£o definido. Pulando teste de integra√ß√£o W&B.")

    print("\nüåê WANDB_MODEL_URL detectado, tentando carregar modelo real...")

    # Carrega o modelo
    classifier = IntentClassifier(examples_file=paths["examples"],
                                  load_model=env_url)
    
    # Verifica se o modelo foi carregado
    if classifier.model is None:
        pytest.fail(
            f"Secrets W&B definidas, mas o modelo falhou ao carregar de {env_url}. "
            "Verifique a API key, a URL do modelo ou se o modelo existe."
        )
        
    print("‚úÖ Modelo carregado do W&B")
    return classifier

@pytest.fixture(scope="module")
def clf_local_trained(paths):
    """
    Fixture de sanidade: Treina um modelo pequeno localmente.
    Usa a sugest√£o de treinar com dados locais para um teste e2e.
    """
    print("\n‚öôÔ∏è Treinando modelo local para testes...")
    
    # Configura√ß√£o m√≠nima para um treino r√°pido
    local_config = Config(
        dataset_name="local_test",
        epochs=2,  # Apenas 2 √©pocas para velocidade
        callback_patience=1,
        validation_split=0.5,
        sent_hl_units=8,  # Modelo pequeno
        wandb_project=None  # N√£o registrar este treino de teste
    )
    
    # Passa o objeto Config e o caminho dos exemplos
    classifier = IntentClassifier(config=local_config, examples_file=paths["examples"])
    classifier.train(tf_verbosity=0)  # Treina silenciosamente
    print("‚úÖ Modelo local treinado.")
    return classifier

@pytest.fixture
def clf_minimal():
    """Classificador leve, sem modelo, para testes de unidade r√°pidos."""
    # Fornece 'codes' para satisfazer o _setup_encoder
    config = Config(
        dataset_name="minimal_test",
        min_words=2,
        codes=["intent_a", "intent_b"]
    )
    return IntentClassifier(config=config)

@pytest.fixture
def clf_with_stopwords(tmp_path):
    """Classificador leve com um arquivo de stopwords tempor√°rio."""
    # tmp_path √© uma fixture nativa do pytest
    stop_words_file = tmp_path / "stopwords.txt"
    stop_words_file.write_text("um\numa\nde\ndo")
    
    config = Config(
        dataset_name="stopwords_test",
        codes=["intent_a", "intent_b"],
        stop_words_file=str(stop_words_file)
    )
    return IntentClassifier(config=config)

# --- Testes de Unidade (R√°pidos) ---
def test_init_fails_without_config_or_model(monkeypatch):
    """Verifica se a inicializa√ß√£o falha se NENHUMA fonte de config/modelo for fornecida."""
    # monkeypatch √© uma fixture nativa do pytest para alterar o ambiente
    monkeypatch.delenv("WANDB_MODEL_URL", raising=False)
    
    with pytest.raises(ValueError, match="config must be a path to a YAML file, a Config object, or None."):
        IntentClassifier()

def test_preprocess_text_lowercase(clf_minimal):
    """Testa a convers√£o para min√∫sculas."""
    result_tensor = clf_minimal.preprocess_text("OI TUDO BEM?")
    assert result_tensor.shape == (1,)
    assert result_tensor.numpy()[0] == b'oi tudo bem?'

def test_preprocess_text_min_words(clf_minimal):
    """Testa o padding de 'min_words'."""
    # A config em 'clf_minimal' define min_words=2
    # O c√≥digo fonte adiciona (min_words + 1) de padding
    result_tensor = clf_minimal.preprocess_text("oi") # Menor que min_words
    assert result_tensor.numpy()[0] == b'<> <> <> '

def test_preprocess_text_stopwords(clf_with_stopwords):
    """Testa a remo√ß√£o de stopwords."""
    result_tensor = clf_with_stopwords.preprocess_text("uma frase de teste")
    assert result_tensor.numpy()[0] == b'frase teste'

# --- Testes de Sanidade Local (M√©dios) ---
def test_local_train_model_created(clf_local_trained):
    """Verifica se o modelo local foi treinado e atribu√≠do."""
    assert clf_local_trained.model is not None
    assert isinstance(clf_local_trained.model, tf.keras.Model)

def test_local_predict_sanity(clf_local_trained):
    """Testa a previs√£o (predict) usando o modelo treinado localmente."""
    clf = clf_local_trained
    top_intent, probs = clf.predict("oi como vai")
    
    assert isinstance(top_intent, str)
    assert top_intent in clf.codes
    assert isinstance(probs, dict)
    assert list(probs.keys()) == list(clf.codes)
    assert sum(probs.values()) == pytest.approx(1.0)

def test_one_hot_encoder_local(clf_local_trained):
    """Valida o one-hot encoder usando o modelo local treinado."""
    clf = clf_local_trained
    enc = clf.onehot_encoder
    codes = list(clf.codes)
    
    assert len(codes) > 1  # Garante que os dados de teste foram carregados
    
    for idx, code in enumerate(codes):
        vec = enc.transform([[code]]).toarray()[0]
        assert len(vec) == len(codes)
        assert vec[idx] == pytest.approx(1.0)
        assert ((vec == 0) | (vec == 1)).all()
        decoded = enc.inverse_transform([vec])[0][0]
        assert decoded == code

# --- Testes de Integra√ß√£o (Lentos) --- podem ser pulados se executar: `pytest -m "not integration"`
@pytest.mark.integration
@pytest.mark.parametrize("clf_wandb, paths", [("confusion", "confusion"), ("clair", "clair")], indirect=["clf_wandb", "paths"])
def test_wandb_model_predicts(clf_wandb, paths):
    """
    Teste de integra√ß√£o real:
    Verifica se o modelo carregado do W&B consegue fazer uma previs√£o.
    """
    print(f"üîé Verificando a previs√£o (predict) do modelo {clf_wandb.config.dataset_name}")
    top_intent, probs = clf_wandb.predict("exemplo qualquer")
    
    print(f"Inten√ß√£o prevista: {top_intent}")
    assert isinstance(top_intent, str)
    assert isinstance(probs, dict)
    assert len(probs) >= 1
    assert top_intent in clf_wandb.codes

@pytest.mark.integration
@pytest.mark.parametrize("clf_wandb, paths", [("confusion", "confusion"), ("clair", "clair")], indirect=["clf_wandb", "paths"])
def test_wandb_model_accuracy_easy_examples(clf_wandb, paths):
    """
    Teste de integra√ß√£o real:
    Verifica a acur√°cia do modelo do W&B em exemplos conhecidos.
    """
    print(f"üåê Usando modelo {clf_wandb.config.dataset_name} para verifica√ß√£o de acur√°cia")
    
    with open(paths["examples"], "r") as f:
        data = yaml.safe_load(f)

    print(f"üìÇ Carregando exemplos de {paths['examples']}")
    samples = []
    for intent_block in data:
        for text in intent_block["examples"]:
            samples.append((text, intent_block["intent"]))
    
    # Pega uma amostra (m√°x 50) para n√£o demorar muito
    samples = samples[:50] 
    texts = [t for t, _ in samples]
    labels = [l for _, l in samples]
    
    preds = clf_wandb.predict(texts)
    pred_labels = [p[0] for p in preds]

    accuracy = sum(p == l for p, l in zip(pred_labels, labels)) / len(labels)
    print(f"üèÜ Acur√°cia na amostra: {accuracy:.2f}")

    # Relat√≥rio e Matriz de Confus√£o
    report = classification_report(labels, pred_labels, zero_division=0)
    print("\nüìÑ Relat√≥rio de Classifica√ß√£o (Amostra):\n" + report)

    all_labels = sorted(set(labels) | set(pred_labels))
    cm = confusion_matrix(labels, pred_labels, labels=all_labels)
    cm_df = pd.DataFrame(cm, index=all_labels, columns=all_labels)
    print("\nüî¢ Matriz de Confus√£o (Amostra):\n" + cm_df.to_string())

    # Define um limite de acur√°cia razo√°vel para o modelo de produ√ß√£o
    assert accuracy >= 0.7